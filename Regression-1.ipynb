{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd5c496-b4e9-4d6f-acf6-f2f7937571a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9847237b-4a9f-40b7-934a-7b8d57a64104",
   "metadata": {},
   "source": [
    "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "\n",
    "- **Simple Linear Regression**: In simple linear regression, there is only one independent variable used to predict a dependent variable. The relationship between the two variables is represented by a straight line.\n",
    "  - **Example**: Predicting a person's salary based on years of experience.\n",
    "\n",
    "- **Multiple Linear Regression**: In multiple linear regression, two or more independent variables are used to predict a dependent variable. The model fits a hyperplane in multidimensional space.\n",
    "  - **Example**: Predicting a person's salary based on years of experience, education level, and age."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f8c5f-b8c9-4371-913c-42e574d3c7fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc04f867-3730-4214-960d-82f552327ec7",
   "metadata": {},
   "source": [
    "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "\n",
    "The assumptions of linear regression are:\n",
    "1. **Linearity**: The relationship between the independent and dependent variables is linear.\n",
    "2. **Independence**: The residuals (errors) should be independent of each other.\n",
    "3. **Homoscedasticity**: The residuals should have constant variance.\n",
    "4. **Normality of residuals**: The residuals should be normally distributed.\n",
    "5. **No multicollinearity**: Independent variables should not be highly correlated.\n",
    "\n",
    "To check these assumptions:\n",
    "- **Linearity**: Use scatter plots between independent and dependent variables.\n",
    "- **Independence**: Use the Durbin-Watson test.\n",
    "- **Homoscedasticity**: Plot residuals vs. fitted values and look for constant spread.\n",
    "- **Normality**: Check residuals using a histogram or Q-Q plot.\n",
    "- **Multicollinearity**: Use the Variance Inflation Factor (VIF) to detect high correlations among independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8caa198-e835-4697-a055-631ed4379880",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aabfcb06-1c0d-4443-a7e9-71b49fabe72a",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "\n",
    "- **Slope**: The slope represents the change in the dependent variable for each unit increase in the independent variable.\n",
    "- **Intercept**: The intercept is the predicted value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "- **Example**: Suppose you have a linear regression model that predicts salary based on years of experience:\n",
    "  - Salary = 30,000 + 5,000 * (Years of Experience)\n",
    "  - **Intercept**: $30,000 (This is the base salary for someone with zero experience).\n",
    "  - **Slope**: $5,000 (For each additional year of experience, the salary increases by $5,000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34384c90-28a1-402d-b2cd-9361881ef445",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "526469ca-e08f-447a-b026-765e7319b65f",
   "metadata": {},
   "source": [
    "### Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "- **Gradient Descent**: It is an optimization algorithm used to minimize the cost function in machine learning models. The algorithm iteratively adjusts the model parameters (weights) in the direction of the negative gradient of the cost function, reducing the error with each step.\n",
    "- **Use in Machine Learning**: Gradient descent is used to train models like linear regression, logistic regression, and neural networks by minimizing the cost function (loss) and finding the optimal parameters that best fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6911fc5-8908-4a6a-bbbd-e98474fd11f9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82f24f49-9bcb-45eb-96e3-9873077a8485",
   "metadata": {},
   "source": [
    "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "\n",
    "- **Multiple Linear Regression**: The model extends simple linear regression by using more than one independent variable to predict the dependent variable. The equation is:\n",
    "  - \\( y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon \\)\n",
    "  \n",
    "  Where:\n",
    "  - \\( y \\) is the dependent variable\n",
    "  - \\( x_1, x_2, ..., x_n \\) are independent variables\n",
    "  - \\( \\beta_0 \\) is the intercept\n",
    "  - \\( \\beta_1, \\beta_2, ..., \\beta_n \\) are the coefficients for the independent variables\n",
    "  - \\( \\epsilon \\) is the error term.\n",
    "\n",
    "- **Difference**: In simple linear regression, there is only one independent variable, whereas multiple linear regression involves multiple independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde27980-214f-4919-8d3f-6efbcdfd9db5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efb034d9-58ec-467d-8451-41a8f2d591da",
   "metadata": {},
   "source": [
    "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "\n",
    "- **Multicollinearity**: Occurs when two or more independent variables in a multiple linear regression model are highly correlated, leading to unreliable estimates of regression coefficients and difficulty in determining the individual effect of each variable.\n",
    "\n",
    "- **Detection**:\n",
    "  - Calculate the **Variance Inflation Factor (VIF)** for each independent variable. A VIF above 5 or 10 indicates multicollinearity.\n",
    "  - Check the **correlation matrix** for highly correlated variables.\n",
    "\n",
    "- **Addressing Multicollinearity**:\n",
    "  - Remove one or more highly correlated variables.\n",
    "  - Use dimensionality reduction techniques such as **Principal Component Analysis (PCA)**.\n",
    "  - Regularization techniques like **Ridge** or **Lasso** regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a0c0f7-6a4b-456b-9b4a-42f42ed76c48",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a9e645b-9f0c-4454-b14e-16f9c11ffa24",
   "metadata": {},
   "source": [
    "### Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "- **Polynomial Regression**: Extends linear regression by adding polynomial terms (squared, cubed, etc.) of the independent variables, allowing for curved relationships between the independent and dependent variables.\n",
    "  - Equation: \\( y = \\beta_0 + \\beta_1x + \\beta_2x^2 + ... + \\beta_nx^n + \\epsilon \\)\n",
    "\n",
    "- **Difference**: Linear regression fits a straight line, while polynomial regression can fit more complex curves to better capture nonlinear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0aebf6-ae3d-4522-9faf-a1cc0c5f569c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b97b291-ef04-4a16-b693-21040763945d",
   "metadata": {},
   "source": [
    "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "- **Advantages**:\n",
    "  - Can model nonlinear relationships between independent and dependent variables.\n",
    "  - Provides a better fit when the data exhibits curvature.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - High-degree polynomials can lead to **overfitting**.\n",
    "  - Model interpretability becomes more difficult as the degree increases.\n",
    "\n",
    "- **When to use**: Polynomial regression is preferred when the relationship between the independent and dependent variables is not linear and can be better captured by a curved fit, such as modeling the growth of bacteria over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13c9ca-251a-4bc1-97bf-4be9333e1520",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
